Raising awareness about a problem can make the problem worse. An example: a movie about how Hollywood owns the world and is teh evilz etc, makes Hollywood stronger because the profits go back to Hollywood. I've seen several such films: they just don't give a fuck because they're smart enough to hide behind ambiguity etc. Oh sure here's our autobiography, change the names of the people and businesses and viola: legal immunity. As for whether or not those movies were an outside source genuinely trying to raise awareness to solve the problem, or if they came from within because the execs are confident enough in their hiding (haha faggots, you're still hiding) that they just made the movie themselves... is unknown to me.