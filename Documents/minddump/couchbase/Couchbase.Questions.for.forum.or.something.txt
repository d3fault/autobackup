1) Key Hotspots - Does CouchBase do auto-balancing of load for a single key? Key "frontpage" for example would be accessed quite often by every client (they could cache it, but then synchronization must be done manually gah). I've read the whitepaper on Google Filesystem, and they had/have this problem too when distributing executables/binaries for a distributed process. Their solution: manually increase the redundancy factor for the key that is a hotspot.
2) View Emit Atomicity - are 2 emits in single View guaranteed to be added to the View's output/index atomically before Reduce processes it? In bank example if one emit is in and the next is not, the balance will not be zero when doing View.Group().Group() to verify all the transactions.
3) How, in the client, to know how often a View was updated? For example, should you use polling to verify the balance in the bank example? Should it be a specialized client (single point of failure!) or should any/every client do this (isn't this inefficient? see question #4)?
4) I know View results/indexes are written to the cluster, but what about filters/reduces? Are they generated only when a client requests them? Seeing as clients can request new filter criteria at runtime, does this mean every filter/reduce result is stored on the cluster? What if we only want it once? I can see scenarios where it would be smart to keep it around, and also scenarios where you'd only want it once (so storing it would be a waste of space). Maybe TTLs in this case (except TTLs wouldn't be calculated until requesting it again... so for the 'only once' case the data would still be there forever!)?
5) Are there any better tap usage examples, including filtering of data? Does the filtering happen on the cluster (efficient) or on the client (inefficient)? Are taps the answer to #3? Can taps even be used with Views? The documentation on Taps is quite lacking. I hope/imagine Taps and their filters can be set up to only stream certain changes based on a key prefix (so you can benefit even more from your 'smart' key design), and that this filtering is done on the cluster?