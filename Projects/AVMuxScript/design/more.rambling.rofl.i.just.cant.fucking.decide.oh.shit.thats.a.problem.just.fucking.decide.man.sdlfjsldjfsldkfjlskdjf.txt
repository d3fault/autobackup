re: Large [binary] files
When syncing from one directory to another, the ideal backup solution would gpg verify on both the sending and receiving side (assuming there's a network in between. If there isn't a network in between the "rsync" command src/dest, one gpg verify is enough). One might argue that it is overkill, and indeed it is if done improperly. BUT, having the file already in memory, a prerequisite to rsync'ing it to begin with, is the majority of the overhead. A gpg verify of that in memory content is cheap/practically-free.

Obviously, this isn't cheap/free unless you have a ton of RAM (commands simplified for readability):
local: gpg verify file
rsync local:file -> remote:file
remote: gpg verify file

A while ago, when I created the MyBrain archives, I got into a habit of making huge as fuck shell commands that used "tee" to calculate sha1sums during copying. I didn't get it to do sha1sum matching (I suck at bash), but that was as easy as looking at the command results at the end of the copy op.

I want something similar to that tee command chaining, but using gpg verify. Maybe rsync has hooks I can setup, but I bet they'll be a pain to tweak properly (ex: if the remote hook "returns 1", I want it's stderr/out to be told to me on the sender side). Rsync's manpage is a beast, I'm skimming it now.

What I want can be accomplished with tee + netcat, providing I send the sig before the file (no biggy). That it doesn't provide encryption is a tiny bit concerning: my files are public so fuck it (and the gpg verify stage kinda almost buys me same shit (eh)), but a proper solution should use ssh.



Guh this all stems from my "import script" task, obviously. I'm scared to break away from git because git provides OVERWRITE PROTECTION. aka: "oh shit the gpg sig doesn't verify... but damnit i overwrite the file so now what?" (in git, I just checkout the old copy). It provides overwrite protection, but at the cost of taking up twice as much space (minimum (although only for non-bare checkouts... so...)).


interesting rsync options (unrelated to above problem unfortunately):
my typical args: -avhhu --partial --progress
--delay-updates (with or without --partial-dir)
--ignore-existing (if pulling in from untrusted source (and in that case, probably combined with --delay-updates/--partial-dir))

OT: I read the wiki page on "ECC" the other day and it made me paranoid as fuck about the [long-term] integrity of my files. Git is enough, yea, but has performance issues for large files guh... which is why I'm researching a gpg sign/verify solution (might as well since it's about as much work as md5/sha1, but buys you much more)


Linus says signing every commit is stupid/overkill. I agree, but fuck it we're talking about an insignificant amount of hdd space. I am tempted to do "ultra sign everything" mode, where the recursive gpg sign tool is used on every file in the repo, the custom-detached-sigs file that recursive tool generates is itself signed (this sig of the custom-detached-sigs file must be excluded from the recursive gpg sign, of course), then sign the git commit as well (which contains the regular repo files, the custom detached signatures file, and the signature of the custom detached signatures file). Definitely overkill, fuck it come at me bro. Oh and of course I should still store last modified timestamps SOMEWHERE (doesn't have to be in custom detached sigs file, so long as it is in the repo as a vanilla file (and generated before the recursive gpg sign (BUT, it is "more efficient" to only recursively iterate the heirarchy once (excluding git's additional recurse lewlies))).

^^This of course is only for text files (well I mean it would work for binaries too, just eh I'm not sure I want to use git for binaries (I "want" to, but there are huge cons to it)).


Another con of git is that I can't split up the repo and store different parts on different drives. Doing so would be confusing regardless, but might be necessary on the short term xD.


To add to the confusion of the import script, currently I'm storing the audio half in git, and the video files simply as files.


Making a "weekly torrent" (automatically, ofc) sounds like it would play nicer with non-git (because what if git updates the files in that torrent?). A "git diff --binary [weekAgoHash] [latestHash]" would be swell/efficient/EASY, but mostly useless to the downloaders of it (unless I provided a tool to apply the patch for them (which implies a tool to download the weekly torrents to begin with (and that sounds like much too much work for a simple "import script" that I just want to fucking KISS and get the fuck done so I can do vanilla piano music fingers and go the fuck outside [more])). NOT using git will force me to "eh don't overwrite these files" (doing so is disasterous fml), so it's easier/saner to say "these are part of this week's torrent")


Reading the git-annex walkthrough, woot I think this is the backukp solution I desire (sure as shit beats custom... I just *hope* it isn't bugged and poof files=/dev/null). I was only 75% sure this was what I'd use, until I got to the line that says "git annex --auto --numcopies=3" (ok it said 2x, but for me I have a bias towards the number 3 :-P). FUCKIN PRO, ALL I HAVE TO DO IS BUY MORE 1tb drives and it will scale out horizontally for me (keeping 3 copies at all times, but who gives a fuck which drives they're on (just need to label them heh)). Eh I'm not sure that attaching a new empty drive will easily migrate to the new drive, but I guess I can sort by largest files and start "dropping" them (which I think triggers a copy?) from there to even out the load. Meh maybe they have a command for this, *keeps reading*. OR MAYBE the better solution is to just not worry about it until a particular drive needs space, and THEN I run the "drop" command (on which files though? guh idfk. I guess sorting by filesize w0rx heh) only as needed (assuming I can predict how much space every op I'm about to execute will need (which... I mean... I should xD)). I'm pretty damn confused on the usage of this, but I can feel it in my bones: this is what I've been trying to design myself! git + large files done right (backup solution protects overwriting ETC, woot).

I'm a bit worried about the security model of git-annex. It has trust models, but since I don't understand it enough in general I can't fully grasp what they even do. It appears as though "untrusted" more means "this drive may fail soon" rather than "the contents of this repo may have been modified by an adversary". It also appears that the trust mode of a given repo (say, an untrusted one) can be raised to "trusted" (above the normal default) by that adversary issuing a simple command. Again, I'm unsure. This system is complex as fuck (but so is designing/coding my own). I just wish git.... wasn't....  retarded.


Hmm, cat file | tee >(gpg --verify) | netcat send (ssh tunneling) | netcat receive | tee >(gpg --verify) > file  ..... easier? Still needs moar versioning+overwrite-protection. Git annex uses simple file permissions for write protection.... so maybe my shit can/should too? I don't really _NEED_ versioning. (and git annex probably still has no protection against root overwriting the file (whereas a true git repo does (the extra copy in the repo))).


I'm not sure I can handle this headache right now (too much to learn), maybe I should just use simple non-git "rsync" + recursive gpg verify stuff manually (but setting them to read-only is a good idea also)... at least I'll KNOW what the fuck is going on...... whereas I don't understand if git-annex sync will (rather, won't ;-P) pull in deletions or even remove-from-history-entirely commands that have been issued on some distant server compromised through some bug in some obscure piece of software that was either installed by default or implicitly installed when I installed something I needed (debian security vulns list is pretty damn active after all).... poof offline copies now deleted. "sync" commands scare me in general, because they imply "delete source so that it matches dest"..... seriously I'd heard of rsync YEARS ago while looking for a way to copy files (ultimate backup solution: STILL NOT SOLVED, rofl)... but the man page _LARGER_ than the size of my dick... and the word "sync" in the command... scared me off. now that I'm using it and understand it (if you want to sync two ways, you run the command twice ;-P), I love it.

Besides, I can always switch to git-annex later. Right now I want something simple that works so I can tape these fucking sensors on my hand and go outside and play the piano.

Hmm what should I do with the timestamps of the binaries (still use .lastModifiedTimestamps file? should I version it just in case? throw it in my text repo? add timestamps to recursive gpg tools? I think I decided I'm going to use a define for timestamps in recursive gpg tools for timestamping behavior, and when a fs timestamp doesn't match what's in the sigs file, error out immediately (this will be my early detection accidental-overwrite-detection, but periodically running recrusive verify will be the ultimate decider))

Weird/funny/OT (but understandable): a read-only file can be copied preserving the read-only attribute, and the copy that you just got finished writing immediately becomes unwriteable right as you finish xD. Seems off lol, but makes sense. The perms are probably updated after the copy finishes anyways (idfk).


And besides, it totally strokes my ego to have all my files signed :) -- but guh an extra file.asc next to every file? no fucking thanks :-P. TODOoptional: verifysinglefile (or list of files, mb the command accepts N args) in custom detached sigs file


Come to think of it, git-annex doesn't provide any overwrite protection if you're using ntfs/fat (where permissions are ignored). In that case, your just rely on copies on other drives (which is the same solution as the cpp script I'm writing now)