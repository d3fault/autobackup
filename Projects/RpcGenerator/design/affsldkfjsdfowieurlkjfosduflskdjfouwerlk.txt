I'm actually a bit confused on how I'd even implement a stream log. OK noobs reading, this isn't entirely true. I get the gist of it. Where I get confused is on MAINTAINING HORIZONTAL SCALABILITY while doing it. It makes me think of an "atomic increment" on a supercomputer/couchbase-cluster (sameshit).... where each and every operation performs the atomic increment. Add to that the CAS in couchbase doesn't even appear to have an atomic increment (gotta just TRY TO GET IT and HOPE YOU DO (it tells you if it succeeds/fails. if it fails you simply keep trying: bullshit strategy)).


Do we "reserve" segments of timelining (specifically, KEYS) for a "smartclient" instance to use? Using of course some greater collaboration scheme. Do we just submit our best-guess timestamp and have the View/Query shit sort it out and verify it? Solving this for a 'business dht' sounds a fuckton easier than solving it for 'd3fault' :-P... though I still don't know a precise solution. Maybe the solution is staring me right in the face: action + timestamp, then the View/Query shit makes a timeline out of it. Does that even have the verification benefits I desire? What is the point of timelining/stream logging. IT IS REPLAYABILITY. So there needs to be an "accept" (phase), "process" (phase that auto-retries), and "result" phase or something... BUT NOW WE'RE TALKING ABOUT 3 SEPARATE EVENTS. What the fuck goes in the timline? All 3? All 3 combined into one? Which timestamp do we use? What if the process phase succeeds but it succeeds too slowly that we think it failed and then we retry it and corrupt our data? This is the same as the "business eating the message" (and/or lying about it) problem in rpc acking. I've come to realize that the "business not responding" error is ultimately the same as the "application not responding" issue that operating systems have to deal with. Killing the application (force close) could result in corrupt data. Applications tend to (if they're good) be coded around such corrupt data. WAL, Stream Logging, Journaling, whatever the fuck you want to call it. I need/want it. Doing it in a scalable fashion is another matter altogether. It's hard enough to do in a single-node environment... and now I want it to be distributed and horizontally scalable. Fuck. Where's that shotgun? Can I continue with MultiServerAbstraction without figuring this out? Are his MessageIDs (re-used!!!) the same, or do we have yet more MessageIDs on another layer (the timelining ones)? Should we combine the two for efficiency? Should MultiServerAbstraction even handle MessageIDs, or is that specific to Rpc? We're talking about "acking". Is that an Rpc functionality or a MultiServerAbstraction functionality. IT IS BOTH. A server abstraction *could* provide acking functionality. An Rpc utility *could* provide acking functionality. I'm inclined to put it in the lower of the two (MultiServerAbstraction), though I can't foresee all future circumstances so I am worried that I may regret it. I'm also worried that I'll be in the middle of implementing it when I discover some reason why it needs to be at Rpc Generator level instead. Perhaps because of the inherent need to know what kind of messages we are dealing with? I could abstract those message types into a simple integer (arbitrary to MultiServer, meaningful to Rpc) to solve that. I... just... don't... know....



Does stream logging / timelining mean I have a synchronous order? Wouldn't that defeat the purpose of an asynchronous system?

Stream logging / timelining... "accepting"... means the CAS race is ultimately delayed by an extra write. Is the CAS race performed at the accept phase or the process phase? If the prior, then nevermind what I just wrote. I simply do not know. I have never implemented a stream log or even a timeline for that matter... I have only researched, analyzed, discussed, designed, etc.. them.



Fuck coding right now
vs.
I want to launch so I can start making music because my life sucks right now