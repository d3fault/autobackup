scaling: the bank server is a couchbase cluster. each cluster node is also an rpc server that functions identically to the next. in this way, any client can call any server and they will all atomically operate on the same dataset. couchbase makes scalability easy :)

race conditions: two clients do not talk to each other (but they can. should??? WAL-type-thing?) before sending their request to the server(s). maybe i shouldn't pre-emptively catch race conditions so god damn aggressively (for now?) and just let the couchbase atomicity handle it. the "set if not exist" thing is handy. so then i don't also need to compare against a 'pending' list (the values, not the pointers!) etc... just let couchbase error out and then respond based on the error (fucking username already exists bitch. owned GET THE FUCK OUT). i actually think i accomplished a lot since the last colon. fuck pending queues and all that nonsense. couchbase atomicity all the way. and this way if the set fails, the next is coming in anyways still and will (maybe) succeed. this still doesn't help me with whether or not to have a cache in teh client and to do a race condition against that... but fuck that's just an optimization anyways. as long as the master copy isn't fucked... WHO GIVES A SHIT. KISS. so NO FUCKING CACHE RACE CONDITION DETECTING [FOR NOW] fuck yea making progress woot

dropped connections: 'message ensuring'. a simple content-agnostic 100/4 method could be used... but oh yea i just remembered: QBitArray has a 32-bit overhead so if i just send a few 'GUARANTEES' at a time it is actually somewhat expensive :(. i could try to manually code the bits in a quint8 or something.. which would give me now a maximum guarantees-per-message (no biggy really, since we'll always have enough messages to hitchhike on)... and a minimum overhead of 8-usedBits. i really don't know. this is why i kinda wanted to use a stream log or a WAL or something idfk. pretty sure TCP just silently fails anyways. god damnit what a piece of fucking shit. if i do some sort of message ensuring, maybe i should keep the pre-streamed QByteArrays around in some "waiting guarantee/verify" queue so i have easy access to them in case i need to re-send them. unlike the above, i am not making much progress here. client->server connection fail (wait what? how do i even detect it?) could just choose a new server... server->client can choose a new client for broadcasts... but actions really do need to be responded to the same client... so the message ensuring idea thing is actually yea something i need but idfk

client crashing: i don't fucking know but it relates to dropped connections
server crashing: ditto