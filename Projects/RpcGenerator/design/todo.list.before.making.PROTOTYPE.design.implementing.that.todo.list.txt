prototype
{
  name = "message stream log enforcing + disconnect recovery"
  
  requirements
  {
    robust-never-shortreading-message-handling
    []
    
    guarantee to client once synchronous (to "serverhelper") call is complete that the message will be delivered and responded to, so long as client does not crash. it is not really synchronous... but it is with respect to the network request we are guaranteeing
    []

    guarantee to client side of network communication (still serverhelper) from server side of network communication (clientshelper and actual business implementation) once
    []

    broadcasts are delivered to each client <-- nvm, just one. and it's optional. if the broadcast was "pending amount detected" then we use the client that performed the getAddFundsKey action as the client to deliver the broadcast to. he also prioritizes notifying the list-of-wt-clients-interested-in-the-data (<- both the requestor of the getAddFundsKey in our case (we should have an "account event notifier" thing at the bottom to notify us of all account-related events. and perhaps even a timeline stream of these events that can be manually purged (with password request) at any time) and the interested parties (that list would be none in that case of "pending amount detected" broadcasts... but say if there's a broadcast related to the purchasing of a specific ad's adSlot (say, while viewing the ad page), even if it was generated by the server as a response to a specific action taking place (do i need a broadcast for ever action? the client should take care of neighbor-notifying as usual (as written below))). ok decision: the processing of interested parties list (lists of wt-appSessions aka "WApplication::sessionId) is done entirely on the client side. he can instantiate broadcasts to his own rpc system (job is purely processing/notifying of abc appdb changes) to his own "clients" (wt nodes. if i want them on the same server and same binary i can set the rpc generator to "cross-thread-mode".
    .....
    broadcasts (and/or vs. events): broadcast to one specific client node known to be online. he will take care of notifying his neighbors and writing to the cluster. in fact, he will pass a reference to his own cluster that he and his neighbor form of the json "document" itself. he will only pass them the key in his broadcast relaying.
    to the server, the broadcast is an atomic write/guaranteed-to-be-"first" to the local server cluster
    .....
    ok so in the event that broadcasts don't have a client, any of the online ones are chosen to take care of the neighbor-notifying (the list of which is stored on the cluster? so can be easily accessed and appended)
    []

    appId would make a create "magic" cookie. a great*
    //[]
    ...
    but then it's too intertwined with the message protocol
    ...
    which is actually good but requires a lot of code change?
    ...
    and may come back to bite me in the ass
    
    actually you know what. appId is a piece of the rpc protocol being defined. not the header, the message itself. it is it's bank account ID, so to speak. of which it has many (one per user) sub-bank-accounts.

    #######################importente
    since i want the same "rpc client" to be able to deliver different "appIds" (one giant binary! (even if modular (because it should be))), i am now saying it's a mandatory part of every BANK SERVER RPC ACTIONS/BROADCASTS. the same server[cluster] "binary" should be able to handle different modes of operations. anything that needs to be remotely considered "secure". bank server is just one mode, which i guess should be an enum on the server's global messaging processing hopefully-never-offline-mode-is-figured-out-by-then. and the bank server "mode" has multiple "appIds" which are just different projects altogether (perhaps i can become a payment processor and allow othere people to have their own appIds. on a dht it should be free but if it's on my own servers... nope? idfk.)
  
    perhaps other rpc servers should operate on a different port? does having it on a different port mean another core can access it at the same time? i doubt it
    
    formal disconnect method... that if not used (network failure, system crash, etc), caches messages until expected reconnect (with eventual timeout... which drops cache like formal disconnect (actually, we want to wait for finish on formal disconnect -- but there might be cases where we drop? sure hope not but never know. timeout probably used here too AND THEN drop))
    []

    agree on like a list of a range of message IDs. use a ring buffer communication to ensure message id buffer is never empty. we ensure between client and server that the agreed upon list is consistent. we only need to send the 'starting message id' and 'ending message id' out of some pre-allocated "pool of message IDs perhaps with some random transformation applied to the order in which we are extracting the messageIDs [for use] out of the message ID pool. and we also need to put a 'verifier' (hash-of-ordering-perhaps? but something cheaper maybe. crc32 would work for example) so say for example we have 100 messageIDs in the pool we will at first request 50 'randomly selected' [the same way by both client + server] and then the ordering of those 50 messages is agreed upon before any of the transactions can take place. receiving a message id that isn't expected immediately triggers a "where is message-xyz (missing)?" in which it can hopefully be resolved by the "synchronous" guarantee and just re-send the message. -----AFTER we have processed 25 of these said message (size is arbitrary but needs to be relatively large so as to always get the next batch of 25 before they are needed... else we block :(), the 50-75 range of messageIDs is chosen out of that 100. and then the 75-100 range is chosen after 50 have been processed. and then they are all reset and up for grabs again in the agreed-upon-random-selection... which is changed from last time (and new 'starting and ending' IDs as well). we could make it so that if we run out of messageIDs and have to block, the poolSize doubles. so the server will auto-scale the messageID pool appropriately. it can communicate with the client and tell it what poolSize to set. the server determines the size because (a) it is being waited upon and (b) it processes multiple clients... so it's values would be higher anyways and therefore a better worst case scenario. the poolSize should also be large enough that the "range agree" messages (their additional cost) is well jusityied justified*. especially since there will be many more of the even smaller and more frequent poolSelectFourthAgreeMessage (that message includes beginning and ending id and the hash verification ---- or perhaps just the hash verification and the "apply-random-transform" (datetime hash (salted?)?) data). rangeAgree is done on client startup/init (one of the two, but before main mode of operation)... AND ALSO when server needs to scale it up to cease blocking (rarely). but poolSelectFourthAgreeMessage happens every poolSize/4 messages no matter what
    []

    there should be a timer that times out and sees if any of the messages have gotten stuck in the business impl -- perhaps also one on the client side (would have to be longer than the business one though, for obvious reasons.. the message has to travel accross the network so it will take longer)
    []

    i envision using bit-arrays as an optimal way of communicating (hopefully by hitchhiking -- but 1-second timeout can send them also) the "request received, here's your guarantee" message. there is no data on it pointing to what message it is.. but that can be figured out by comparing lists. the bit array is arbitrarily large.. it might even be able to be zero(??? message behind 'fast-self-guaranteeing-message' ???). fast-self-guaranteeing-message: you mark a messageId and the guarantee waits for < 1 second and so isn't "where's this message checked upon" but the first message it can "hitch a ride on" is itself. so like the very first message would be this. aside from that, we want to shove as many bits available onto the bit array whenever we have a message to send. for this reason, the bitstream should be processed and accounted for BEFORE processing the message we are hitchhiking on... because the first bit in the array could be the guarantee for the message it's hitchhiking on... and we can't process a message that hasn't even been guaranteed first. figured out: it CAN be zero. the guarantee bit array can be zerolen. example: send 5 messages that are all 5 processed faster than 1 second... and all 5 processed faster (added to guarantee bit array before) than any of the messages can be responded to. the size-5 bit array/guarantee will hitchhike on the first message back, and the 2-5th do not have any bit array guarantees (but COULD for example have 6+ if they came in in the meantime)
    
    the client and server should both handle intermitent disconnections/reconnections as well (have a multiple minute timeout for this). a server looks at how far he is in the stream log and simply request from where he left off. the client then gives him the requests, whether they were previously guaranteed or not... the server says "i shut down but am back give me everything from here" and the client does. the guarantee means nothing when the server CRASHES... but the client (serverhelper) will still try to fulfill his guarantee to the business client impl. perhaps they should be allowed to cancel a pending guarantee (maye this happens in/near global notification area?) if it's been pending for multiple seconds (server crash?). if they leave the browser open or browse away then the request will/should go through... eventually

    messageIDs, used in stream log. client IDs, each with their own stream log. appID, an rpc-business-server-specific sub-identification means. (bank account (with sub-bank-accounts) in bank server example)
    []

    what about multiple rpc server business impls? wouldn't that be a different chunk of generated code and so operate on a different port/protocol? it doesn't necessarily have to... but what should i decide to do? i suppose the appId could be used solely to select which business impl, but then the appId becomes part of the header, not the message body like i plan.
  }
  special functionality for debugging
  {
    randomly drop 1/10 messages over the network (client -> server. client makes guarantee to the code using the client's api) (<-- the "synchronous guarantee" i was referring to earlier is actually asynchronous... because Wt has to use invokeMethod to call anything in Qt land (for example, my rpc appdb client). but it is still synchronous in so far as it has not left the server yet (that may or may not be true synce (rofl) we've asynchronously sent the network message/request (i guess it isn't necessarily a request, but it's easiest to think about that way. perhaps i should randomly drop 1/10 broadcasts too? i can't drop them in the business impl because that makes no sense (TODOreq: or does it? if it does, handle that error case "lost broadcasts -- aka ones business was supposed to send but didn't")) regardless))
    
    randomly drop 1/10 messages in rpc business (server -> client. server makes guarantee to the client)

    TODOreq: think about what should happen to the first random drop if the client (either the client impl or the server helper business) goes offline after the guarantee and the request hasn't propagated to the server yet
    TODOreq: similar to above, think about what should happen to the second random drop if the server/business should cease to execute    
  }
  optional enhancements
  {
    blocking detection of poolSize scaling. if the server ever blocks to wait for more messageIDs in the message stream, then we double poolSize in hopes that it doesn't happen again. initially, just start off with 100/4 and don't worry about scaling automatically. can manually if i need to (probably won't, but might)
    []
    ^^^this is a chance to my dispenser logic.. but is also similar to it. i still need the dispenser/owner thread logic stuff, so my work won't be wasted. this is just an upgrade to it... especially the stream log / matching / verifying part of it
  }
}