From the server thread, we've either:
a) Haven't gotten the message
b) Gotten the message and dispatched it to business (and made note of it)
c) Gotten it and it's response from the business and sent it back to the client
d) It is still on the kernel's socket read buffer

All 3 of those are done within a single unit of execution (each), so we know there is no other states for the messages
Actually I'm adding (d) now, no idea how to handle that one TODOreq handle it




Perhaps I just need to do the 3 Use Case diagrams in dia? Perhaps doing them will be a breeze because I'll have the perfect perspective (design level).

Use Case #1 - Clean Disconnect
Use Case #2 - Dirty Disconnect, detected by new client connection with matching cookie
Use Case #3 - Dirty Disconnect, detected by server through socket errors/etc while [attempting-to] receive message
Use Case #4 - Dirty Disconnect, detected by server through socket errors/etc while [attempting-to] send message
Use Case #5 - Dirty Disconnect, re-connect fails hello phase (server down, service crash (TODOreq: perhaps provide a way for business to report service crashes so the server can know to not accept more? or perhaps he announces that he is down? or perhaps both (rejection AND announcement?)), etc) -- TODOreq: what do i do with pending messages at this point??? the state of our shit could be compromised, fuck



TODOreq: We will never get another message after a Clean Disconnect message (we do not rely solely on tcp disconnect, we use a special Disconnect message and then wait for the messages received before the Disconnect Message to finish) because the client knows better than to send them, so (d) doesn't always factor in


Oh shit just thought of use case #4 (and modifying #3 as they are similar). But hmm, won't those be signals that can come at any time? So can I even differentiate them? Perhaps I'm erroneously associating them with the send/receive "unit of execution(s)". Perhaps they are their own!


There is also this:
a) We either haven't received any messages
b) All received messages are in business pending
c) All received messages are in lazy ack state
d) Some in pending, some in lazy ack state
e) Every variation of above, with some in kernel socket read buffer // for use case "dirty disconnect dupe cookie detected", we don't care if there are messages in the socket read buffer because the client will simply send them again on the new connection. also for the other dirty disconnect, we should err on the side of caution by not reading the kernel buffer once we see the socket error. the message will be re-sent later and will fall under the category of 'request lost on network'



Basically I need to have an m_Connected bool that is defaulting to false and very carefully set. If I see pretty much any error, I set it to false. Clean disconnects will need special considerations. Meh I think there's way more to it than that. We have to put it in some m_UglyDisconnectMayReconnect hash/queue etc, and TODOreq it still needs to be able to receive Actions (TODOreq: we should make sure it doesn't receive any Broadcasts while in this state) as they come back from the Business, and whether or not we put it in the "ack pending" list for simplicity or into it's own special "haven't-yet-sent-because-ugly-dc-detected"/queue is tbd.

Ok so methinks #3/#4 Use Cases above are the same again: fuck it. They still both need to be CONSIDERED [probably?]. TODOreq consider implications





Random: Right now I visualize myself on a board on a ball and I'm balancing back and forth (just left and right). This code is pivotal but actually not all that hard. I guess what I mean is that this code is KEY. With proper designs and understanding, however, it isn't necessarily that complex (einstein's law). The ball is on some the peak of "pyramid" and/or "triangle" which represents a large chunk of code (the rpc generator?)





TODOreq: Who is IN CHARGE OF making sure no more Actions come in after the Disconnect Message? It makes sense that the Client is in charge of it, but should my Server be able to handle them and reject them? TODOoptional/optimization(idfk which. TODOrobust? I could see TODOrobust keeping me infinitely and stupidly busy (I do have indefinite time though... soo...hmmm.... ^_^)): to make it more robust we could reject them with a special message code... but for MY OWN CODE I can just make sure that the Client doesn't send any after the Disconnect Message. KISS for now


TODOreq: I need to detect the time when "Disconnect Message has been Received" && "The last business pending message was sent back in a response", so at that point the Server will send a "Disconnect Acknowledged You Can Now TCP Disconnect" (does this mean I am in a special state while awaiting that TCP Disconnect?). Also TODOreq: should an ugly disconnect (no tcp disconnect, so like socket errors etc) at this point still give me a clean disconnect? I'd say yes, but any time BEFORE entering that "AwaitingTCPDisconnect" state will result in ugly disconnect. Also it isn't always "TCP Disconnect" (local socket), but just however the underlying socket does a disconnect (maybe ugly is the only way for some?)

TODOreq: after a clean disconnect, should we release the resources? Or even an ugly disconnect that times out (or merges (only some part of it can be released at this point, probably the QIODevice, but not the protocol knower since that is re-used post-merge))

There are obviously a ton of considerations and I wonder if I'm missing any? Maybe they'll become apparent if/when I make those Use Case diagrams??





TODOreq: after a connection has timed out (in our rpc generator) and we clear it's queue etc, what if it connects again and sends the actions again with the retry bit set (or whatever the reconnect bit / retry-actions-bkoz-reconnect is called)? the actions may or may not have been processed, but we have no record of them. should we at this point know not to process them because the 'retry'^^^ bit is set and we don't see them? we might see this as a 'request lost over network' false positive, so perhaps the connect-specific-retries need their own name for this very reason?
^^^it is reasonable to assume that a client won't send a retry action message after the timeout (because i can hard code the timeout (but there is still a race condition so NVM it is NOT reasonable)), but we can't control other clients (oh wait yes we _ARE_ other clients because it's autogenerated code woot) so eh: we definitely need to handle retry-bkoz-connection-reset-actions where they've been purged because of timeout. how do we handle it??? no fucking clue :-(

race condition:
client

hmmm now i'm thinking there isn't a race condition. idfk tbh. since a client can't send a message when he isn't connected, it's impossible for him to have sent a message during the timeout period [that will arrive after the timeout (that was the race condition)].

do i perhaps need to tell the client whether or not his cookie was used for a merge? a simple boolean in the hello phase could let him know, and from there we'd know what to do with messages:
cookie not used for merge: we didn't make timeout period, thus all previous pending actions are ??????????????? wtf ????????????????
cookie used for merge: we made the timeout period, therefore all previous pending actions can be re-requested (with a bit set i guess)

the same race condition re-appears: connect(oldCookie) sent just before timeout, received on server after timeout <--- has same problem as a faulty client that sends old cookie after timeout regardless (rare)

fml && KISS: <thinks>
if we get a socket error on the client before we get a response to a create bank account action (we're bound to get a socket error here eventually, though we won't know where we get it first it still happens in both client and server (right?)), we don't even know that the server got the request! if we reconnect and the reconnecting fails a few times and then it succeeds just before the timeout (in our view/thread/machine) and just after the timeout (on server view/thread/machine), the server will claim no knowledge of it even if it had already received it and acted upon it!! extending the timeout does zilch (except lessens chances). maybe we need to make sure that our 'ok start sending bro' is received within the client's perspective of the timeout? that is actually a shit requirement/guarantee and has it's own race conditions now. should all 'pending' actions that the client has be "GenericError'd" whenever the server responds that he is NOT using our cookie (indicating we missed the timeout (or even a service error/restart tbh TODOreq: consider that))? should I tell the user that the action might have completed successfully but it also might not have? i mean that's the truth but idfk wtf to do/say at this point. i feel like i need 'promise'-ing in my protocol somewhere, but gah that would change the design soooooooo fucking much :-/. there could also be a timeline thingy that each server instance checks against (so we can safely try on another server if need be), but now i'm getting back into the whole 'fs-integration' problem and what the fuck. i am pretty damn sure this same abstract problem occurs in many different domains. the "oops we just erased that shit" race condition or something xD. fuck it, what is a usable solution? even if we just 'make note of it' and record all the conditions of what happened (generic error but without saying the maybe/maybe-not clause to the _user_) so we can debug it better later.. that might be ok. but shit, i know the conditions already! the conditions of the race condition. does this mean i cross my fingers and hope it never happens?

hmm what the fuck? aren't action timeouts going to be significantly less than connection timeouts? won't we have returned a generic error way before the connection timeout even factors in (TODOreq: make sure they never overlap or get too near that a race condition occurs (equation: connection-timout minus action-timeout needs to be [much] greater than client/server latency))

TODOreq: i think there's another hard to solve problem relating to broadcasts also... like if server gets socket error we won't know if client got it or not. we'd be awaiting the hard ack, but it might never come. at some point i guess we'd select another connection (if one's available) and then try the broadcast again? right here is a potential collision and possibly a race condition. the "clients" might receive the same broadcast twice! does rpc generator need to solve that, or is that a higher level's problem (one with fs/timeline access)? or are broadcasts much less significant/important (they aren't modifying the state of the client, are they? they have already modified the servers' dht and all that remains is notifying the clients etc -- it shouldn't be, and might not be, a big deal if that notification is received twice. but tbh, idfk)

TODOreq: the 'purge unfinished hellos' code that i haven't added or designed might fuck with my overall connection timeout/reconnect/merge scheme.... so to solve that i just ned to make sure the purge unfinished hellos timeout is way higher than the regular connection timeout. there might be other things to consider, however. think hard about it

all actions will generically error out after roughly 3 seconds (might expand to 5?)
reconnect/merge within 3 seconds: re-send (do i use the same timeout? might need extension? or be tough about it? idfk)
reconnect/merge after 3 seconds: they already generically error'd out so fuck it/them. user has to retry them? but they might have gone through? wtf????
reconnect/merge after 30 second connection timeout (even though we send the 'connect' before we thought the timeout happened -- server needs to tell us if he used the cookie i guess): all actions have already generically error'd out so this isn't even a concern <---- JACKPOT


i think i need to modify my use cases to account for trying an action on another server, except i think that design should (a) - be part of a higher level design not relating to rpc generator and (b) - it is ridiculously hard to know if the server accepted a request so err ehh uhh should i try it again on ANOTHER one? i mean, is it even a good idea? should i use multiple server connections for redundancy or just for round robin? obviously it would be BEST to use it for redundancy as well, but is that feasible atm?

either way, once the client gets a socket error (or a clean disconnect *request*), actions should use other servers from then on (what if none available? bitch please i'm building a highly available system so i mean if that's the case then just generically error :-P. there will be lots of servers! durr)